{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac7e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet openai tiktoken python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a823c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LLM ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env (NOT committed)\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY in your environment\"\n",
    "\n",
    "# OpenAI client (no key in code, uses env var)\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Hello LLM ðŸ‘‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9503152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model_hint: str = \"gpt-4.1-mini\") -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in a given text using tiktoken. Falls back to cl100k_base.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model_hint)\n",
    "    except Exception:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def estimate_cost(tokens: int, model: str = \"gpt-4.1-mini\", io: str = \"input\") -> float:\n",
    "    \"\"\"\n",
    "    Estimate USD cost for `tokens` using example $/1M rates.\n",
    "    `io` is \"input\" or \"output\".\n",
    "    \"\"\"\n",
    "    prices = {\n",
    "        \"gpt-4.1-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4.1\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"gpt-5\": {\"input\": 10.00, \"output\": 30.00},\n",
    "    }\n",
    "    rate_per_token = prices[model][io] / 1_000_000\n",
    "    return tokens * rate_per_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b13a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from inside VS Code! ðŸ‘‹\n",
      "\n",
      "Here are three key topics about Retrieval-Augmented Generation (RAG):\n",
      "\n",
      "1. **Integration of Retrieval with Generation:** RAG combines a retrieval system (like a search engine or database) with a generative model (such as GPT) to provide more accurate, context-aware, and up-to-date responses.\n",
      "\n",
      "2. **Use of External Knowledge Sources:** Instead of relying solely on pre-trained knowledge, RAG fetches relevant documents or data in real-time to enhance the generation process.\n",
      "\n",
      "3. **Applications and Performance:** RAG is used in tasks like open-domain question answering, customer support, and knowledge management, improving the factual correctness and relevance of generated text.\n",
      "\n",
      "If you'd like, I can help you set up an environment in VS Code for experimenting with RAG as well!\n",
      "\n",
      "Q&A: The two main steps in RAG before generation are indexing chunks in a vector database using embeddings and retrieving the top-k most similar chunks for a given query.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Say hello from inside VS Code and list three key topics about RAG.\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=user_prompt,\n",
    ")\n",
    "\n",
    "print(response.output[0].content[0].text)\n",
    "\n",
    "# OPTIONAL: tiny Q&A over a small text (simulating 'context window' usage)\n",
    "context = \"\"\"RAG = Retrieve-Augmented Generation. Key parts:\n",
    "- Indexing chunks in a vector DB (embeddings like OpenAI text-embedding-3-large).\n",
    "- Retrieving top-k most similar chunks for a query.\n",
    "- Prompting the LLM with the retrieved chunks + user question.\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = f\"\"\"You are a helpful assistant. Use ONLY the context below.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: What are the two main steps in RAG before generation?\n",
    "Answer in one sentence:\"\"\"\n",
    "\n",
    "resp2 = client.responses.create(model=\"gpt-4.1-mini\", input=qa_prompt)\n",
    "print(\"\\nQ&A:\", resp2.output[0].content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
